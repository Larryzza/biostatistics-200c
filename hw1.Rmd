---
title: "Biostat 200C Homework 1"
author: "Zian ZHUANG"
output: pdf_document
  # ioslides_presentation: default
subtitle: Due Apr 16 @ 11:59PM
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(ggplot2)
library(tidyverse)
library(faraway)
library(gtsummary)
```

## Q1. Binomial Distribution

Let $Y_i$ be the number of successes in $n_i$ trials with 

$$Y_i\sim Bin(n_i, \pi_i),$$
where the probabilities $\pi_i$ have a Beta distribution

$$ \pi_i \sim Beta(\alpha,\beta).$$
The probability density function for the Beta distribution is $f(x;\alpha, \beta) = x^{\alpha-1}(1-x)^{\beta -1}/B(\alpha,\beta)$ for $x\in [0,1], \alpha>0, \beta>0$, and the beta function $B(\alpha,\beta)$ defining the normalizing constant required to ensure that $\int_0^1f(x;\alpha, \beta) = 1$. Let $\theta = \alpha/(\alpha +\beta)$, show that

\begin{itemize}
\item[a.] $E(\pi_i) = \theta$ 
$$\begin{aligned} 
E(\pi_i) &= \int \pi_i*f(\pi_i)d\pi_i\\
 &= \int \pi_i*\pi_i^{\alpha-1}(1-\pi_i)^{\beta -1}/B(\alpha,\beta)d\pi_i\\
 &= B(\alpha,\beta)^{-1}\int \pi_i^{(\alpha+1)-1}(1-\pi_i)^{\beta -1}d\pi_i\\
 &= B(\alpha+1,\beta)*B(\alpha,\beta)^{-1}\int B(\alpha+1,\beta)^{-1}*\pi_i^{(\alpha+1)-1}(1-\pi_i)^{\beta -1}d\pi_i\\
 &= B(\alpha+1,\beta)*B(\alpha,\beta)^{-1}*1\\
 &= \Gamma(\alpha+1)\Gamma(\beta)/\Gamma(\alpha+1+\beta)/(\Gamma(\alpha)\Gamma(\beta))*\Gamma(\alpha+\beta)\\
 &= \alpha/(\alpha +\beta)\\
 &= \theta 
\end{aligned} $$

\item[b.] $Var(\pi_i) = \theta(1-\theta)/(\alpha +\beta+1) = \phi\theta(1-\theta)$ 
Firstly we can calculated $E(\pi_i^2)$
$$\begin{aligned} 
E(\pi_i^2) &= \int \pi_i^2*f(\pi_i)d\pi_i\\
 &= \int \pi_i^2*\pi_i^{\alpha-1}(1-\pi_i)^{\beta -1}/B(\alpha,\beta)d\pi_i\\
 &= B(\alpha,\beta)^{-1}\int \pi_i^{(\alpha+2)-1}(1-\pi_i)^{\beta -1}d\pi_i\\
 &= B(\alpha+2,\beta)*B(\alpha,\beta)^{-1}\int B(\alpha+1,\beta)^{-1}*\pi_i^{(\alpha+2)-1}(1-\pi_i)^{\beta -1}d\pi_i\\
 &= B(\alpha+2,\beta)*B(\alpha,\beta)^{-1}*1\\
 &= \Gamma(\alpha+2)\Gamma(\beta)/\Gamma(\alpha+2+\beta)/(\Gamma(\alpha)\Gamma(\beta))*\Gamma(\alpha+\beta)\\
 &= \alpha*(\alpha+1)/(\alpha +1+\beta)*(\alpha +\beta)\\
 &= \theta(\alpha+1)/(\alpha +1+\beta) 
\end{aligned} $$
Then we can obtain $Var(\pi_i)$
$$\begin{aligned} 
Var(\pi_i) &= E(\pi_i^2)-E(\pi_i)^2\\
&=((\alpha +1)\alpha(\alpha+\beta)-\alpha^2(\alpha+\beta+1))/(\alpha+\beta+1)(\alpha+\beta)^2\\
&=(\alpha\beta)/(\alpha+\beta)^2/(\alpha +1+\beta) \\
&=\theta(1-\theta)/(\alpha +\beta+1)= \phi\theta(1-\theta)
\end{aligned} $$

\item[c.] $E(Y_i) = n_i\theta$ 

$$\begin{aligned} 
E(Y_i) &= E_{\pi_i}(E_{Y_i}(Y_i|\pi_i))\\
&=E_{\pi_i}(n_i*\pi_i)\\
&=n_i*E(\pi_i)\\
&=n_i*\theta
\end{aligned} $$

\item[d.] $Var(Y_i) =n_i \theta(1-\theta)[1+(n_i-1)\phi]$ so that  $Var(Y_i)$ is larger than the Binomial variance (unless $n_i=1$ or $\phi = 0$).

$$\begin{aligned} 
Var(Y_i)&=E_{\pi_i}(Var(Y_i|\pi_i))+Var_{\pi_i}(E(Y_i|\pi_i))\\
&=E_{\pi_i}(n_i*\pi_i*(1-\pi_i))+Var_{\pi_i}(\pi_i*n_i)\\
&=n_i*(E(\pi_i)-E(\pi_i^2))+n_i^2*\phi\theta(1-\theta)\\
&=n_i*(\theta-\theta(\alpha+1)/(\alpha +1+\beta))+n_i^2*\phi\theta(1-\theta)\\
&=n_i*(\theta(1-(\alpha+1)/(\alpha +1+\beta)))+n_i^2*\phi\theta(1-\theta)\\
&=n_i*(\theta*\beta/(\alpha +1+\beta))+n_i^2*\phi\theta(1-\theta)\\
&=n_i*(\theta*(1-\theta)(1-\phi))+n_i^2*\phi\theta(1-\theta)\\
&=n_i \theta(1-\theta)[1+(n_i-1)\phi]
\end{aligned} $$

\end{itemize}
\newpage
## Q2. (ELMR Chapter 3 Exercise 1) 

A case-control study of esophageal cancer in Ileet-Vilaine, France. 
```{r}
data(esoph)
#help(esoph)
```

### a. Plot the proportion of cases against each predictor using the size of the point to indicate the number of subject as seen in Figure 2.7. Comment on the realtionships seen in the plots.

Solution:
```{r}
plot_data <- esoph %>%
  mutate(proportion=ncases/(ncontrols+ncases))

ggplot(plot_data, aes(agegp, proportion))+
  geom_point(aes(size = ncontrols+ncases),alpha = 7/10)+
  ylab("Proportion of cases")+
  xlab("Age group, years")+
  theme_bw()

ggplot(plot_data, aes(alcgp, proportion))+
  geom_point(aes(size = ncontrols+ncases),alpha = 7/10)+
  ylab("Proportion of cases")+
  xlab("Alcohol consumption, gm/day")+
  theme_bw()

ggplot(plot_data, aes(tobgp, proportion))+
  geom_point(aes(size = ncontrols+ncases),alpha = 7/10)+
  ylab("Proportion of cases")+
  xlab("Tobacco consumption, gm/day")+
  theme_bw()
```

### b. Fit a binomial GLM with interactions between all three predictors. Use AIC as a criterion to select a model using the `step` function. Which model is selected?

Solution:
```{r, message = FALSE}
lmod = glm(cbind(ncases, ncontrols) ~ agegp*alcgp*tobgp, 
           family = binomial, data=esoph)
minmod = glm(cbind(ncases, ncontrols) ~ 1, 
           family = binomial, data=esoph)
lmod_step = step(lmod, direction = "both")
lmod_step = step(lmod, direction = "backward")
lmod_step = step(minmod, direction = "forward", scope=~agegp*alcgp*tobgp)
lmod_step %>%
 tbl_regression(intercept = TRUE)
```

We choose a model by AIC in three Stepwise Algorithms ("both", "backward", "forward"). All of results provides the same best model. Thus, we selected `cbind(ncases, ncontrols) ~ agegp + alcgp + tobgp` as the best model. 

### c. All three factors are ordered and so special contrasts have been used appropriate for ordered factors involving linear, quadratic and cubic terms. Further simplification of the model may be possible by eliminating some of these terms. Use the `unclass` function to convert the factors to a numerical representation and check whether the model may be simplified.

Solution:
```{r, message = FALSE}
lmod = glm(cbind(ncases, ncontrols) ~ unclass(agegp) + unclass(alcgp) 
           + unclass(tobgp), family = binomial, data=esoph)
lmod_unclass = step(lmod, direction = "both")
lmod_unclass = step(lmod, direction = "backward")
lmod_unclass = step(minmod, direction = "forward",
             scope=~unclass(agegp)+unclass(alcgp)+unclass(tobgp))
lmod_unclass %>%
 tbl_regression(intercept = TRUE)
```

We choose a model by AIC in three Stepwise Algorithms ("both", "backward", "forward"). All of results provides the same best model. Thus, we selected `cbind(ncases, ncontrols) ~ unclass(agegp) + unclass(alcgp) + unclass(tobgp)` as the best model. 

### d. Use the summary output of the factor model to suggest a model that is slightly more complex than the linear model proposed in the previous question.

Solution:
```{r, message = FALSE}
#refer to original factor model
lmod = glm(cbind(ncases, ncontrols) ~ agegp*alcgp*tobgp, 
           family = binomial, data=esoph)
#final model
lmod_final = glm(cbind(ncases, ncontrols) ~ unclass(alcgp) + 
                   agegp + unclass(tobgp), family = binomial, data=esoph)
lmod_final %>%
 tbl_regression(intercept = TRUE)
```
According to the summary output of the factor model, we found that quadratic term of age group is significant within the 95% confidence interval. In addition, tobgp and alcgp only have significant linear terms. Thus, we kept agegp as ordered categorical variable and unclassed alcgp and tobgp.

### e. Does your final model fit the data? Is the test you make accurate for this data?

Solution:
```{r}
# test the deviance
pchisq(lmod_final$deviance, lmod_final$df.residual, lower = FALSE)

df <- esoph %>%
  mutate(proportion=ncases/(ncontrols+ncases)) %>%
  mutate(weight=(ncontrols+ncases))
predprob <- predict(lmod_final, type = "response")

# Pearson chi-square statistic
px2 <- sum((df$ncases - df$weight*predprob)^2 / 
             (df$weight*predprob*(1 - predprob)))
pchisq(px2, lmod_final$df.residual, lower.tail = FALSE)

# Hosmer-Lemeshow test statistic
df_binned <- df %>%
  mutate(predprob = predict(lmod_final, type = "response"),
         linpred = predict(lmod_final, type = "link"),
         bin = cut(linpred,
                   breaks = unique(quantile(linpred, (1:100) / 101)))) %>%
  group_by(bin) %>%
  summarize(y = sum(ncases),
            avgpred = mean(predprob),
            count = sum(weight)) %>%
  mutate(se_fit = sqrt(avgpred * (1 - avgpred) / count)) 
df_binned %>%
  ggplot(mapping = aes(x = avgpred, y = y / count)) +
  geom_point() +
  geom_linerange(mapping = aes(ymin = y / count - 2 * se_fit,
                               ymax = y / count + 2 * se_fit), alpha = 0.5) +
  geom_abline(intercept = 0, slope = 1) +
  labs(x = "Predicted probability", y = "Observed proportion")

# Hosmer-Lemeshow test
hlstat <- with(df_binned, sum((y - count * avgpred)^2 / 
                                (count * avgpred * (1 - avgpred))))
# J
nrow(df_binned)
# p-value
pchisq(hlstat, nrow(df_binned) - 1, lower.tail = FALSE)
```

We conducted pearson chi-squre test on the deviance D, Pearson chi-square statistic and Hosmer-Lemeshow test statistic. All of them present large p-value, indicating that the model has an adequate fit.

### f. Check for outliers in your final model.

Solution:
```{r}
df %>%
  mutate(devres = residuals(lmod_final, type = "deviance"))%>%
  mutate(linpred = predict(lmod_final, type = "link")) -> df
halfnorm(hatvalues(lmod_final))
halfnorm(cooks.distance(lmod_final))
```
According to the hatvalues and cooks.distance plots, we identified potential high influential observations (13, 67, 78). 

Then we print out outliers and check:
```{r}
df %>%
  slice(c(13, 67, 78))
```


### g. What is the predicted effect of moving one category higher in alcohol consumption?

Solution:
```{r}
coefs <- coef(lmod_final)
odds = exp(coefs[2] * 1)
round(as.numeric(odds),2)
```
According to the results, we know that the risk of moving one category higher in alcohol consumption, the risk would be 92% higher.


### h. Compute a 95\% confidence interval for this predicted effect.

Solution:
```{r}
confint(lmod_final)
odds_lower = exp(0.4888562 * 1)
odds_upper = exp(0.8205436 * 1)

#95% confidence interval
round(as.numeric(c(odds_upper, odds_lower)), 2)
```

The estimated 95% confidence interval for this effect of moving one category higher in alcohol consumption is (1.63, 2.27).
